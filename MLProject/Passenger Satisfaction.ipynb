{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #handling data\n",
    "import pandas as pd #handling data\n",
    "\n",
    "import seaborn as sns #Data Visualisation\n",
    "import matplotlib.pyplot as plt #Data Visualisation\n",
    "\n",
    "import warnings # filter warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import RandomizedSearchCV #import GridSearch from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('datasets/train.csv') # load the train dataset\n",
    "df_train.head() #show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('datasets/test.csv') # load the test dataset\n",
    "df_test.head() #show first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info() #training dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df_train.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info() #testing dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['Unnamed: 0',\"id\", 'Gate location','Food and drink','Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Checkin service', 'Inflight service', 'Cleanliness' ],axis=1, inplace=True) # Drop Columns in training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['Unnamed: 0',\"id\", 'Gate location','Food and drink','Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Checkin service', 'Inflight service', 'Cleanliness' ],axis=1, inplace=True) # Drop Columns in testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum() #check for null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum() #check for null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe() #statistics of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.fillna(15.178678,inplace=True) #Fill all null values with mean of 'Arrival Delay in Minutes' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum() #check to find null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape # number of rows and columns training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True) # Drop rows with null values\n",
    "df_test.shape #number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True,inplace=True) #Re-index test dataset\n",
    "df_test #current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum() #check for duplicate values in training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates() #drop duplicate values in training set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.duplicated().sum() #checks for duplicate values in testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop_duplicates() #drop duplicate values in testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLORATORY DATA ANALYSIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation of the correlation\n",
    "#heatmap representing the correlation betweeen each of the features (training)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_train.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualisation of the correlation\n",
    "#heatmap representing the correlation betweeen each of the features (training)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_test.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Satisfaction Training\n",
    "print(df_train.satisfaction.value_counts())\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"In train data\")\n",
    "sns.countplot(data = df_train, x = \"satisfaction\" , palette = \"rainbow\", order = [\"satisfied\",\"neutral or dissatisfied\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Satisfaction Testing\n",
    "print(df_test.satisfaction.value_counts())\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"In train data\")\n",
    "sns.countplot(data = df_train, x = \"satisfaction\" , palette = \"rainbow\", order = [\"satisfied\",\"neutral or dissatisfied\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Training \n",
    "print(df_train.Gender.value_counts())\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"In test data\")\n",
    "sns.countplot(data = df_test, x = \"Gender\" , palette = \"rainbow\", order = [\"Male\",\"Female\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Training\n",
    "print(df_train.Gender.value_counts())\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"In train data\")\n",
    "sns.countplot(data = df_train, x = \"Gender\" , palette = \"rainbow\", order = [\"Male\",\"Female\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Testing \n",
    "print(df_test.Gender.value_counts())\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.subplot(122)\n",
    "plt.title(\"In test data\")\n",
    "sns.countplot(data = df_test, x = \"Gender\" , palette = \"rainbow\", order = [\"Male\",\"Female\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('In training set')\n",
    "sns.countplot('Class',data=df_train,hue='Customer Type',palette=['Black', 'Blue'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('In testing set')\n",
    "sns.countplot('Class',data=df_test,hue='Customer Type',palette=['Black', 'Blue'])\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical plots onto a FacetGrid (Training)\n",
    "graph_train = sns.catplot(\"Age\", data=df_train, aspect=5.0, kind='count', order=range(6, 86), palette=['Red', 'Black'], hue='satisfaction' )\n",
    "graph_train.set_ylabels('Satisfaction of Flight Passengers')\n",
    "\n",
    "# Categorical plots onto a FacetGrid (Testing)\n",
    "graph_test = sns.catplot(\"Age\", data=df_test, aspect=5.0, kind='count', order=range(6, 86), palette=['Black', 'Red'], hue='satisfaction')\n",
    "graph_test.set_ylabels('Satisfaction of Flight Passengers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(y = k, data = df_train[\"Flight Distance\",\"Age\"], , ax = axs[idx]) \n",
    "df_train.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder - To convert The Catagorical data in Features (Training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OneHotEncoder to convert the catagorical variables to numeric values (training set) \n",
    "from sklearn.preprocessing import OneHotEncoder # Import OneHotEncoder\n",
    "oh = OneHotEncoder(drop='first', dtype=np.int64) #Instance #drop first column # only need n-1 columns\n",
    "dfn = df_train[[ 'Gender', 'Customer Type', 'Type of Travel', 'Class', 'Inflight wifi service', \n",
    "                'Departure/Arrival time convenient', 'Ease of Online booking',  'Online boarding',  'Baggage handling']] #take a subset of df_train\n",
    "dfn = oh.fit_transform(dfn).toarray() #fit_transform on subframe to make it a sparse matrix\n",
    "dfn = pd.DataFrame(dfn) #converts matrix to dataframe\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, dfn], axis=1) #concating the features\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.drop([ 'Gender', 'Customer Type', 'Type of Travel', 'Class', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',  'Online boarding',  'Baggage handling'],axis=1,inplace=True) #dropping converted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train #current training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder - To convert The Catagorical data in Features (Testing dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OneHotEncoder to convert the catagorical variables to numeric values (testing set)\n",
    "from sklearn.preprocessing import OneHotEncoder # Import OneHotEncoder\n",
    "oh = OneHotEncoder(drop='first', dtype=np.int64)  #Instance #drop first column # only need n-1 columns\n",
    "dfn = df_test[[ 'Gender', 'Customer Type', 'Type of Travel', 'Class', 'Inflight wifi service', \n",
    "                'Departure/Arrival time convenient', 'Ease of Online booking',  'Online boarding',  'Baggage handling']] #take a subset of df_train\n",
    "dfn = oh.fit_transform(dfn).toarray() #fit_transform on subframe to make it a sparse matrix\n",
    "dfn = pd.DataFrame(dfn) #converts matrix to dataframe\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test, dfn], axis=1) #concating the features\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.drop([ 'Gender', 'Customer Type', 'Type of Travel', 'Class', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',  'Online boarding',  'Baggage handling'],axis=1,inplace=True) #dropping the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test #current training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dummies Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['satisfaction'].value_counts() #number of values for each catagory before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get dummies - Training\n",
    "satisfied = pd.get_dummies(df_train[\"satisfaction\"],drop_first=True) #converting categorical features\n",
    "satisfied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, satisfied], axis=1) #concating the categorical features\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.drop(['satisfaction'],axis=1,inplace=True) #dropping the original column\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['satisfied'].value_counts() #number of values for each catagory after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET Dummies - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['satisfaction'].value_counts() #number of values for each catagory before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get dummies - Testing\n",
    "satisfied = pd.get_dummies(df_test[\"satisfaction\"],drop_first=True) #converting catagorical features\n",
    "satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test, satisfied], axis=1) #concating the categorical features\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.drop(['satisfaction'],axis=1,inplace=True) #dropping the original column\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['satisfied'].value_counts() #number of values for each catagory after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training data\n",
    "X_train = df_train.iloc[:,:-1] #all rows and columns except final column\n",
    "y_train = df_train.iloc[:, -1] #all rows but only of final column\n",
    "#testing data\n",
    "X_test= df_test.iloc[:,:-1] #all rows and columns expect final column\n",
    "y_test =df_test.iloc[:, -1] #all rows but only of final column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap representing the correlation betweeen each of the features (training)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_train.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap representing the correlation betweeen each of the features (testing)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_test.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rows and Columns in each instance\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SCALING METHOD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SCALING ROBUST SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "sc = RobustScaler() #instance\n",
    "X_train_Rscaled = sc.fit_transform(X_train) # fit and transform the training features\n",
    "#we only scale the features-X not the labels-y\n",
    "X_test_Rscaled = sc.transform(X_test)  # not fit this method #transform the testing features - to make sure the same scaling is applied to both testing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SCALING MINMAX SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler() #instance\n",
    "X_train_MMscaled = sc.fit_transform(X_train) # fit and transform the training features\n",
    "#we only scale the features-X not the labels-y\n",
    "X_test_MMscaled = sc.transform(X_test)  # not fit this method #transform the testing features - to make sure the same scaling is applied to both testing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST Algorithm (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#function for training the algorithm with training data\n",
    "# \n",
    "# output: Algorithm Accuracy of testing and training\n",
    "def fitAndTest(X_train, X_test, y_train, y_test):\n",
    "    model = XGBClassifier()\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    accuracy = model.score(X_train, y_train)\n",
    "    print(\"Training Score: {:.3f}\".format(accuracy))\n",
    "    \n",
    "    model.fit(X_test, y_test)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(\"Testing Score: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AND TESTING ACCURACY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy before scaling - XGBoost\n",
    "print(\"Training and Testing Score without feature scaling: \")\n",
    "fitAndTest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Greens\", fmt = \".0f\",\n",
    "                 xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model,X_train,y_train,cv=5, scoring=\"accuracy\")\n",
    "print(\"Cross validaion scores: \")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_average = scores.mean()\n",
    "print(\"average model accuracy: {:.3f}\".format(XGB_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier() #instance for algorithm\n",
    "print(model.get_params().keys()) #print parameters in the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model tunning\n",
    "params_xgboost = {\n",
    "    \n",
    "    \"n_estimators\": [5, 10, 15, 20, 25],\n",
    "    \"eval_metric\": ['logloss'],\n",
    "    \"learning_rate\": [0.2, 0.4, 0.6, 0.8, 1],\n",
    "    \"max_depth\": [2, 4, 6, 8, 10],\n",
    "}\n",
    "\n",
    "print(params_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRIDSEARCH for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV #import GridSearch from sklearn\n",
    "xgboost_grid = GridSearchCV(model,params_xgboost, cv = 5, scoring = 'accuracy',return_train_score=False)\n",
    "\n",
    "xgboost_grid.fit(X_train,y_train) #training the algorithm\n",
    "pd.DataFrame(xgboost_grid.cv_results_)[['mean_test_score', 'params']] \n",
    "print(\"Best model accuracy: {:3f}\".format(xgboost_grid.best_score_)) #best model accuracy\n",
    "print(\"Best HyperParameter Values: \", xgboost_grid.best_params_) #best Hyperparameters values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMSEARCH for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV #import GridSearch from sklearn\n",
    "xgboost_rand = RandomizedSearchCV(model, params_xgboost, cv=5, scoring='accuracy', n_iter=10, random_state=0, return_train_score=False)\n",
    "xgboost_rand.fit(X_train, y_train) #training the algorithm\n",
    "pd.DataFrame(xgboost_rand.cv_results_)[['mean_test_score', 'params']]\n",
    "print(\"Best model accuracy: {:3f}\".format(xgboost_rand.best_score_)) #best model accuracy\n",
    "print(\"Best HyperParameter Values: \", xgboost_rand.best_params_) #best Hyperparameters values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Hyperparameters XGBoost\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model = XGBClassifier(eval_metric= 'logloss', learning_rate = 0.4, max_depth = 8, n_estimators = 25)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Greens\", fmt = \".0f\",\n",
    "                 xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model,X_train,y_train,cv=5, scoring=\"accuracy\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_average = scores.mean()\n",
    "print(\"average model accuracy: {:.3f}\".format(XGB_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION Algorithm (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#function for training the algorithm with training data\n",
    "# \n",
    "# output: Algorithm Accuracy of testing and training\n",
    "\n",
    "def fitAndTest(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    y_test_pred = model.predict(X_test) #Prediction calculation\n",
    "\n",
    "    print(\"Predicted values: \",y_test_pred)\n",
    "    print(\"Actual values: \",y_test)\n",
    "\n",
    "    accuracy = model.score(X_train, y_train)\n",
    "    print(\"Training Score: {:.3f}\".format(accuracy))\n",
    "    \n",
    "    model.fit(X_test, y_test)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(\"Testing Score: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING AND TESTING ACCURACY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy before scaling - LogisticRegression \n",
    "print(\"Before Feature Scaling\")\n",
    "fitAndTest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy after scaling - LogisticRegression\n",
    "print(\"After Feature Scaling\")\n",
    "fitAndTest(X_train_MMscaled, X_test_MMscaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Logistic Regression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model = LogisticRegression() #Using the hyperparameters suggest by 'GridSearch'\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred)) #prints the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Greens\", fmt = \".0f\",\n",
    "                 xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model,X_train,y_train,cv=5, scoring=\"accuracy\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_average = scores.mean()\n",
    "print(\"average model accuracy: {:.3f}\".format(lr_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "print(model.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_logistic = {\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty' : ['l1'],\n",
    "    'max_iter': list(range(1,31)),\n",
    "    'C' : [ 0.01, 0.1, 1.0, 10, 100 ]   \n",
    "}\n",
    "\n",
    "print(params_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRIDSEARCH for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(model,params_logistic,cv=5,scoring=\"accuracy\",return_train_score=False)\n",
    "grid.fit(X_train,y_train)\n",
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'params']]\n",
    "print(\"Best model accuracy: {:3f}\".format(grid.best_score_))\n",
    "print(\"Best HyperParameter Values: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMSEARCH for Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand = RandomizedSearchCV(model,params_logistic,cv=5,random_state=0, n_iter=10, scoring=\"accuracy\",return_train_score=False)\n",
    "rand.fit(X_train,y_train)\n",
    "pd.DataFrame(rand.cv_results_)[['mean_test_score', 'params']]\n",
    "print(\"Best model accuracy: {:3f}\".format(rand.best_score_))\n",
    "print(\"Best HyperParameter Values: \", rand.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model = LogisticRegression(C= 0.1, max_iter= 30, penalty= 'l1', solver= 'liblinear') #Using the hyperparameters suggest by 'GridSearch'\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred)) #prints the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Greens\", fmt = \".0f\",\n",
    "                 xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model,X_train,y_train,cv=5, scoring=\"accuracy\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_average = scores.mean()\n",
    "print(\"average model accuracy: {:.3f}\".format(lr_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP CLASSIFIER - Algorithm 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def fitAndTest(X_train, X_test, y_train, y_test):\n",
    "#function for training the algorithm with training data\n",
    "# \n",
    "# output: Algorithm Accuracy of testing and training\n",
    "    model = MLPClassifier(random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    print(\"Training Accuracy: {:.3f}\".format(train_accuracy))\n",
    "\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    print(\"Testing Accuracy: {:.3f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING AND TESTING ACCURACY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy before scaling - MLP\n",
    "print(\"Before Feature Scaling\")\n",
    "fitAndTest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy after scaling - MLP\n",
    "print(\"After Feature Scaling\")\n",
    "fitAndTest(X_train_Rscaled, X_test_Rscaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifcation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Reds\", fmt = \".0f\", xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation for MLP:\n",
    "#to calculate average model accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = MLPClassifier()\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy') #to store the value of each fold\n",
    "print(scores)\n",
    "scores_train = scores.mean()\n",
    "print(\"{:.3f}\".format(scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores_train = scores.mean()\n",
    "print(\"Average model accuracy {:.3f}\".format(average_scores_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(max_iter=30)\n",
    "print(model.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'learning_rate_init' : [10, 1, 0.1, 0.01, 0.001],\n",
    "    'activation' : ['identity','logistic','tanh','relu'],\n",
    "    \"hidden_layer_sizes\": [(1,),(10,),(15,),(20,),(30,)]\n",
    "\n",
    "}\n",
    "params_grid = parameters\n",
    "print(params_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRID SEARCH FOR MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#MLP - Grid Search CV\n",
    "MLP_grid = GridSearchCV(model,params_grid,cv=5,scoring=\"accuracy\",return_train_score=False)\n",
    "MLP_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(MLP_grid.cv_results_)[[\"mean_test_score\",\"params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best model's accuracy: {:.3f}\".format(MLP_grid.best_score_))\n",
    "print(\"Used values:\",MLP_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM SEARCH FOR MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#MLP - RANDOM Search CV\n",
    "rand = RandomizedSearchCV(model,params_grid,cv=5,scoring=\"accuracy\",return_train_score=False, n_iter=10,random_state=0)\n",
    "rand.fit(X_train,y_train)\n",
    "pd.DataFrame(rand.cv_results_)[[\"mean_test_score\",\"params\"]]\n",
    "print(\"The best model's accuracy: {:.3f}\".format(rand.best_score_))\n",
    "print(\"Used values:\",rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=df_train.corr().round(2)\n",
    "sns.set(rc={'figure.figsize': (15,10)})\n",
    "sns.heatmap(data=matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model = MLPClassifier(activation = 'logistic', hidden_layer_sizes = (30,), learning_rate_init = 0.001) #Using the hyperparameters suggest by ''\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_matrix(label,feature):\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.heatmap(confusion_matrix(label,feature), annot = True, cmap = \"Reds\", fmt = \".0f\", xticklabels = ['satisfied', 'not satisfied'], yticklabels = ['satisfied','not satisfied'])\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation for MLP:\n",
    "#to calculate average model accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = MLPClassifier()\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy') #to store the value of each fold\n",
    "print(scores)\n",
    "scores_train = scores.mean()\n",
    "print(\"{:.3f}\".format(scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores_train = scores.mean()\n",
    "print(\"Average model accuracy {:.3f}\".format(average_scores_train))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1637f3f7394f9d6c34ffc5be246d053302d61edef761b89d5cb24c791c674ba"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
